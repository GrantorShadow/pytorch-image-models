    # TODO fix when needed
    # def forward_deep_prompt(self, embedding_output):
    #     attn_weights = []
    #     hidden_states = None
    #     weights = None
    #     B = embedding_output.shape[0]
    #     num_layers = self.vit_config.transformer["num_layers"]

    #     for i in range(num_layers):
    #         if i == 0:
    #             hidden_states, weights = self.encoder.layer[i](embedding_output)
    #         else:
    #             if i <= self.deep_prompt_embeddings.shape[0]:
    #                 deep_prompt_emb = self.prompt_dropout(self.prompt_proj(
    #                     self.deep_prompt_embeddings[i-1]).expand(B, -1, -1))

    #                 hidden_states = torch.cat((
    #                     hidden_states[:, :1, :],
    #                     deep_prompt_emb,
    #                     hidden_states[:, (1+self.num_tokens):, :]
    #                 ), dim=1)


    #             hidden_states, weights = self.encoder.layer[i](hidden_states)

    #         if self.encoder.vis:
    #             attn_weights.append(weights)

    #     encoded = self.encoder.encoder_norm(hidden_states)
    #     return encoded, attn_weights
            









# img_size: Union[int, Tuple[int, int]] = 224,
#                  patch_size: Union[int, Tuple[int, int]] = 16,
#                  in_chans: int = 3,
#                  num_classes: int = 1000,
#                  embed_dim: int = 768,
#                  depth: int = 12,
#                  num_heads: int = 12,
#                  mlp_ratio: float = 4.,
#                  qkv_bias: bool = True,
#                  drop_rate: float = 0.,
#                  attn_drop_rate: float = 0.,
#                  drop_path_rate: float = 0.,
#                  embed_layer: Callable = PatchEmbed,
#                  norm_layer: Optional[LayerType] = None,
#                  act_layer: Optional[LayerType] = None,













    # # now first create ViT and extend 
    #     super().__init__(img_size=img_size, patch_size=patch_size, in_chans=in_chans, num_classes=num_classes,
    #                      embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,
    #                      qkv_bias=qkv_bias, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,
    #                      drop_path_rate=drop_path_rate, embed_layer=embed_layer,
    #                      norm_layer=norm_layer, act_layer=act_layer)
        

    #     # load state dict from pretrained ViT
    #     if basic_state_dict is not None:
    #         self.load_state_dict(basic_state_dict, False)

    #     self.VPT_type = VPT_type
    #     if VPT_type == "Shallow":
    #         self.Prompt_Tokens = nn.Parameter(torch.zeros(depth, Prompt_Token_num, embed_dim))
    #     else: # deep Prompt
    #         self.Prompt_Tokens = nn.Parameter(torch.zeros(1, Prompt_Token_num, embed_dim))
        
    # def Freeze(self):
    #     """ freeze the params of the ViT
    #     """
    #     # set all the params to false
    #     for param in self.parameters():
    #         param.requires_grad = False
        
    #     # only unfreeze this token
    #     self.Prompt_Token.requires_grad = True

    # def get_prompt(self):
    #     """get prompt in embeddable format in a state dict
    #     """
    #     prompt_state_dict = {'Prompt_Tokens': self.Prompt_Tokens}
    #     return prompt_state_dict
    
    # def load_prompt(self, prompt_state_dict):
    #     """
    #     load the prompt in the GPU
    #     """
    #     if self.Prompt_Tokens.shape == prompt_state_dict['Prompt_Tokens'].shape:
    #         # first we move the prompt to the CPU then the GPU
    #         Prompt_Tokens = nn.Parameter(prompt_state_dict['Prompt_Tokens'].cpu())
    #         Prompt_Tokens.to(torch.device(self.Prompt_Tokens.device))

    #         self.Prompt_Tokens = Prompt_Tokens

    #     else:
    #         print('\n !!! cannot load prompt')
    #         print('shape of model req prompt', self.Prompt_Tokens.shape)
    #         print('shape of model given prompt', prompt_state_dict['Prompt_Tokens'].shape)
    #         print('')

    # def forward_features(self, x: torch.Tensor) -> torch.Tensor:
    #     return super().forward_features(x)
     
